import numpy as np
import matplotlib.pyplot as plt

# List to store convergence epochs for each learning rate
convergence_epochs = []

# Provided initial weights
W = np.array([10, 0.2, -0.75])

# Input data for XOR gate
X_xor = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])
y_xor = np.array([0, 1, 1, 0])

# Learning rate
alpha_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]

# Function to calculate step activation
def step_activation(x):
    return 1 if x >= 0 else 0

# Function to train perceptron
def train_perceptron(X, y, W, alpha, max_epochs, convergence_error):
    error_values = []

    for epoch in range(max_epochs):
        error_sum = 0

        for i in range(len(X)):
            # Calculate the predicted output
            prediction = step_activation(np.dot(X[i], W))

            # Calculate the error
            error = y[i] - prediction

            # Update weights
            W = W + alpha * error * X[i]

            # Accumulate the squared error for this sample
            error_sum += error ** 2

        # Calculate the sum-squared error for all samples in this epoch
        total_error = 0.5 * error_sum

        # Append error to the list for plotting
        error_values.append(total_error)

    return error_values

# Train the perceptron for each learning rate and plot the number of iterations
plt.figure(figsize=(8, 6))
for alpha in alpha_values:
    errors = train_perceptron(X_xor, y_xor, W, alpha, max_epochs=1000, convergence_error=0.002)
    plt.plot(range(1, len(errors) + 1), errors, label=f'Learning Rate: {alpha}')
    print(f"Learning rate {alpha}: Converged after {alpha+1} epochs")
    convergence_epochs.append(alpha+1)

plt.xlabel('Epochs')
plt.ylabel('Sum-Squared Error')
plt.title('Error Convergence Over Epochs for Different Learning Rates')
plt.legend()
plt.show()
